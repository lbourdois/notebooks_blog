{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutoriel : Application de BERT via DistillBERT\n",
    "\n",
    "Ce tutoriel est une traduction de celui proposé par Jay Alammar.  \n",
    "Son blog : https://jalammar.github.io/  \n",
    "L'article de son blog pour lequel il a créé le notebook :  https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/  \n",
    "Une traduction française de cet article :  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "izA3-6kffbdT"
   },
   "source": [
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-sentence-classification.png\" />\n",
    "\n",
    "Dans ce notebook, nous allons utiliser un modèle de deep learning pré-entrainé pour traiter certains textes. Nous utiliserons ensuite les résultats de ce modèle pour en effectuer une classification. Le texte étant une liste de phrases tirées de critiques de films. Nous nous limiterons à une classification bianire où les phrases seront classées soient comme \"positives\" soit comme \"négative\".\n",
    "\n",
    "\n",
    "\n",
    "## Classification de sentiments\n",
    "\n",
    "Notre but est de créer un modèle qui prend une phrase (tout comme celles de notre ensemble de données) et produit soit 1 (indiquant que la phrase porte un sentiment positif) ou 0 (indiquant que la phrase porte un sentiment négatif). On peut penser que ça ressemble à ça :\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/sentiment-classifier-1.png\" />\n",
    "\n",
    "Sous le capot, le modèle est en fait composé de deux modèles :\n",
    "\n",
    "* DistilBERT qui traite la phrase et transmet les informations qu’il en extrait au modèle suivant. DistilBERT est une version plus petite de BERT développée et open source par l’équipe de HuggingFace. C’est une version plus légère et plus rapide de BERT (40% plus léger et 60% plus rapide) et ayant des performances semblables (à 97%).\n",
    "* Le modèle suivant, une basique régression de scikit learn, qui prend en compte le résultat du traitement de DistilBERT et classe la phrase comme positive ou négative (1 ou 0, respectivement). =\n",
    "\n",
    "Les données que nous passons entre les deux modèles sont un vecteur de taille 768. On peut imaginer ce vecteur comme un embedding de la phrase que l’on peut utiliser pour la classification.\n",
    "\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/distilbert-bert-sentiment-classifier.png\" />\n",
    "\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Le jeu de données que nous utiliserons dans cet exemple est [SST2](https://nlp.stanford.edu/sentiment/index.html), qui contient des phrases de critiques de films, chacune étant labélisée positive (a la valeur 1) ou négative (a la valeur 0):\n",
    "\n",
    "\n",
    "<table class=\"features-table\">\n",
    "  <tr>\n",
    "    <th class=\"mdc-text-light-green-600\">\n",
    "    sentence\n",
    "    </th>\n",
    "    <th class=\"mdc-text-purple-600\">\n",
    "    label\n",
    "    </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      1\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      apparently reassembled from the cutting room floor of any given daytime soap\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      0\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      they presume their audience won't sit still for a sociology lesson\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      0\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      this is a visually stunning rumination on love , memory , history and the war between art and commerce\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      1\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      jonathan parker 's bartleby should have been the be all end all of the modern office anomie films\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      1\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "## Installation de la librairie transformers \n",
    "Commençons par installer la bibliothèque de transformateurs huggingface pour que nous puissions charger notre modèle NLP d'apprentissage profond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "colab_type": "code",
    "id": "To9ENLU90WGl",
    "outputId": "4b46c997-c16c-4141-eaf2-e7aa7da6d3a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/f9/51824e40f0a23a49eab4fcaa45c1c797cbf9761adedd0b558dab7c958b34/transformers-2.1.1-py3-none-any.whl (311kB)\n",
      "\r",
      "\u001b[K     |█                               | 10kB 15.1MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 20kB 2.2MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 30kB 3.2MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 40kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 51kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 61kB 3.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████▍                        | 71kB 3.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 81kB 4.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 92kB 4.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▌                     | 102kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 112kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 122kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 133kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▊                 | 143kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 153kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 163kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 174kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 184kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 194kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 204kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 215kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 225kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 235kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▎      | 245kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 256kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 266kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 276kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▍  | 286kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 296kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 307kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 317kB 3.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 53.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.14)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
      "Collecting regex\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
      "\u001b[K     |████████████████████████████████| 645kB 39.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
      "\u001b[K     |████████████████████████████████| 860kB 48.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: botocore<1.14.0,>=1.13.14 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.14)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.14->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.14->boto3->transformers) (2.6.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=cb6e175a99d8f14f69f593b734ea73303c23a7dbdc694be65435cfeebd1f3124\n",
      "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sentencepiece, regex, sacremoses, transformers\n",
      "Successfully installed regex-2019.11.1 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "fvFvBLJV0Dkv",
    "outputId": "140119e5-4cee-4604-c0d2-be279c18b125"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQ-42fh0hjsF"
   },
   "source": [
    "## Importation du dataset\n",
    "Utilisons pandas pour lire le jeu de données et le charger dans un dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cyoj29J24hPX"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dMVE3waNhuNj"
   },
   "source": [
    "Pour des raisons de performance, nous n'utiliserons que 2 000 phrases de l'ensemble du jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gTM3hOHW4hUY"
   },
   "outputs": [],
   "source": [
    "batch_1 = df[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRc2L89hh1Tf"
   },
   "source": [
    "On peut demander à pandas combien de phrases sont labélisées \"positives\" (valeur 1) et combien sont labélisées \"négatives\" (valeur 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "jGvcfcCP5xpZ",
    "outputId": "4c4a8afc-1035-4b21-ba9a-c4bb6cfc6347"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1041\n",
       "0     959\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1[1].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_MO08_KiAOb"
   },
   "source": [
    "## Charger le modèle BERT pré-entrainé "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "q1InADgf5xm2",
    "outputId": "dbc52856-4d52-42f8-8a74-a89944280a02"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 2649246.11B/s]\n",
      "100%|██████████| 492/492 [00:00<00:00, 284634.15B/s]\n",
      "100%|██████████| 267967963/267967963 [00:03<00:00, 72728701.55B/s]\n"
     ]
    }
   ],
   "source": [
    "# Pour DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## Vous souhaitez utiliser BERT au lieu de distilBERT? \n",
    "## Décommenter la ligne suivante:\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Chargement du mdoele pré-entrainé et du tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZDBMn3wiSX6"
   },
   "source": [
    "Actuellement, la variable `model` contient un modèle distilBERT pré-entraîné : une version de BERT qui est plus petite, mais beaucoup plus rapide et nécessite beaucoup moins de mémoire.\n",
    "\n",
    "\n",
    "## Modèle #1 : Préparation du dataset\n",
    "Avant de pouvoir donner nos phrases à BERT, nous avons besoin d'un traitement minimal pour les mettre dans le format requis.\n",
    "\n",
    "#### Tokenisation\n",
    "Notre première étape consiste à tokenizer les phrases , c'est à dire les décomposer en mots et en sous-mots dans le format avec lequel BERT est à l'aise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dg82ndBA5xlN"
   },
   "outputs": [],
   "source": [
    "tokenized = batch_1[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHwjUwYgi-uL"
   },
   "source": [
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-tokenization-2-token-ids.png\" />\n",
    "\n",
    "### Padding (rembourrage)\n",
    "\n",
    "Après la tokenization, `tokenized` est une liste de phrases : chaque phrase est représentée comme une liste de tokens. Nous voulons que BERT traite nos exemples en une seule fois (en un seul lot). C'est plus rapide comme ça. Pour cette raison, nous devons remplir toutes les listes à la même taille, de sorte que nous puissions représenter l'entrée comme un tableau à deux dimensions, plutôt que comme une liste de listes (de différentes longueurs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URn-DWJt5xhP"
   },
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mdjg306wjjmL"
   },
   "source": [
    "Notre jeu de données est maintenant dans la variable `padded`, nous pouvons voir ses dimensions ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jdi7uXo95xeq",
    "outputId": "be786022-e84f-4e28-8531-0143af2347bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 59)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sDZBsYSDjzDV"
   },
   "source": [
    "### Masking\n",
    "Si nous envoyons directement `padded` à BERT, ce serait un peu confus. Nous devons créer une autre variable pour lui dire d'ignorer (masquer) le rembourrage que nous avons ajouté quand il traite son entrée. Voilà ce qu'est un masque d'attention :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4K_iGRNa_Ozc",
    "outputId": "d03b0a9b-1f6e-4e32-831e-b04f5389e57c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 59)"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jK-CQB9-kN99"
   },
   "source": [
    "## Model #1: Et maintenant place au Deep Learning!\n",
    "Maintenant que notre modèle et nos données sont prêts, lançons notre modèle !\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-tutorial-sentence-embedding.png\" />\n",
    "\n",
    "La fonction `model()` fait passer nos phrases par BERT. Les résultats du traitement seront retournés dans `last_hidden_states`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39UVjAV56PJz"
   },
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FoCep_WVuB3v"
   },
   "source": [
    "Coupons seulement la partie de la sortie dont nous avons besoin. C'est le résultat correspondant au premier token de chaque phrase.  \n",
    "La façon dont BERT procède à la classification des phrases est d'ajouter un token appelé \" [CLS] \" (pour classification) au début de chaque phrase. La sortie correspondant à ce token peut être considérée comme un embedding pour toute la phrase.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png\" />\n",
    "\n",
    "Nous les enregistrerons dans la variable `features`, car elles serviront de caractéristiques à notre modèle de régression logitique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9t60At16PVs"
   },
   "outputs": [],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_VZVU66Gurr-"
   },
   "source": [
    "Les labels indiquant quelle phrase est positive et négative sont assignés dans la variable `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JD3fX2yh6PTx"
   },
   "outputs": [],
   "source": [
    "labels = batch_1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iaoEvM2evRx1"
   },
   "source": [
    "## Modele #2: Découpage Entraînement/Test\n",
    "\n",
    "Séparons maintenant notre jeu de données en un jeu d'entraînement et un jeu de test (même si nous utilisons 2 000 phrases du jeu d'apprentissage SST2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddAqbkoU6PP9"
   },
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B9bhSJpcv1Bl"
   },
   "source": [
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-train-test-split-sentence-embedding.png\" />\n",
    "\n",
    "### [Bonus] Grille de recherche des paramètres optimaux\n",
    "Nous pouvons plonger dans la Régression Logistique directement avec les paramètres par défaut de Scikit Learn. Mais parfois cela vaut la peine de chercher la meilleure valeur du paramètre C, qui détermine la force de régularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cyEwr7yYD3Ci"
   },
   "outputs": [],
   "source": [
    "# parameters = {'C': np.linspace(0.0001, 100, 20)}\n",
    "# grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
    "# grid_search.fit(train_features, train_labels)\n",
    "\n",
    "# print('best parameters: ', grid_search.best_params_)\n",
    "# print('best scrores: ', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KCT9u8vAwnID"
   },
   "source": [
    "Nous entraînons maintenant le modèle LogisticRegression(). Si vous avez choisi d'opter pour la grille de recherche, vous pouvez insérer la valeur de C dans la déclaration du modèle (par exemple `LogisticRegression(C=5.2)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "gG-EVWx4CzBc",
    "outputId": "9252ceff-a7d0-4359-fef9-2f72be89c7d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3rUMKuVgwzkY"
   },
   "source": [
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-training-logistic-regression.png\" />\n",
    "\n",
    "## Evaluation du Model #2\n",
    "\n",
    "Dans quelle mesure notre modèle permet-il de classer les phrases ? L'un des moyens consiste à vérifier la précision par rapport à l'ensemble de données test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iCoyxRJ7ECTA",
    "outputId": "cfd86dea-5d16-476c-ab9b-47cbee3a014f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.824"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "75oyhr3VxHoE"
   },
   "source": [
    "Est-cet un bon score ? A quoi peut-on le comparer ? Regardons d'abord un classificateur factice :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lnwgmqNG7i5l",
    "outputId": "0042aed2-4fa8-4fa0-bf25-fdef70a10aac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier score: 0.527 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier()\n",
    "\n",
    "scores = cross_val_score(clf, train_features, train_labels)\n",
    "print(\"Dummy classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Lg4LOpoxSOR"
   },
   "source": [
    "Notre modèle est donc nettement plus performant qu'un classificateur factice. Mais comment se compare-t-il aux meilleurs modèles ?\n",
    "\n",
    "\n",
    "## Benchmark des scores SST2 \n",
    "Le [score obtenant actuellement le meilleur taux](http://nlpprogress.com/english/sentiment_analysis.html) pour ce dataset est de **96.8%**. DistilBERT peut être entrainé pour améliorer le score pour cette tache de classification  – un processus appellé **fine-tuning**. Cela met à jour les poids de BERT pour permettre d'obtenir une meilleure performance. Le DistilBERT \"fine-tuné\" permet d'obtenir un score de **90.7%**. Le BERT Large permet d'obtenir quant à lui un score de **94.9%**.\n",
    "\n",
    "\n",
    "Et c'est tout ! C'est un bon premier contact avec BERT.  \n",
    "L'étape suivante serait de consulter la documentation et d'essayer le [fine-tuning] (https://huggingface.co/transformers/examples.html#glue). Vous pouvez également revenir en arrière et passer de distilBERT à BERT et voir comment cela fonctionne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EJQuqV6cnWQu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "A Visual Notebook to Using BERT for the First Time.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
